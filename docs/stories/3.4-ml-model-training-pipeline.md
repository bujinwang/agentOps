# Story 3.4: Machine Learning Model Training and Deployment Pipeline

## Status
Draft

## Story
**As a** system administrator,
**I want** an automated ML model training pipeline that learns from historical lead conversion data,
**so that** the lead scoring system continuously improves and provides accurate predictions.

## Context
The application has basic lead scoring logic, but it's static and rule-based. A machine learning approach can learn patterns from historical data to better predict which leads will convert. This story implements a complete ML pipeline from feature engineering to model deployment and monitoring.

## Acceptance Criteria
1. System extracts features from lead data automatically
2. Training pipeline runs weekly on historical lead conversion data
3. Multiple models are trained and compared (Logistic Regression, Random Forest, XGBoost)
4. Best performing model is automatically selected based on validation metrics
5. Model performance metrics are tracked (AUC-ROC, precision, recall, F1)
6. Feature importance is calculated and stored for interpretability
7. Models are versioned and stored in model registry
8. New model is deployed to production after passing performance thresholds
9. A/B testing framework compares new model against production model
10. Model predictions are logged for monitoring and debugging
11. Model drift is detected and alerts are sent when performance degrades
12. Retraining is triggered automatically when drift exceeds threshold
13. Training data includes at least 1000 leads with known outcomes
14. Model inference latency is under 100ms for single prediction
15. Model explanations are provided for each prediction (SHAP values)

## Tasks / Subtasks

- [ ] **Task 1: Feature Engineering Pipeline** (AC: 1, 13)
  - [ ] Create feature extraction service
  - [ ] Implement BANT (Budget, Authority, Need, Timeline) feature extraction
  - [ ] Extract engagement features (interaction count, recency, frequency)
  - [ ] Calculate lead velocity features (time between status changes)
  - [ ] Extract demographic features (location, property preferences)
  - [ ] Implement feature normalization and scaling
  - [ ] Create feature validation to handle missing values
  - [ ] Store features in lead_features table with versioning
  - [ ] Implement feature caching for performance
  - [ ] Create feature engineering documentation

- [ ] **Task 2: Training Data Preparation** (AC: 13)
  - [ ] Create training data extraction query
  - [ ] Define positive examples (converted leads)
  - [ ] Define negative examples (lost/stale leads)
  - [ ] Implement data validation (min 1000 samples, balanced classes)
  - [ ] Create train/validation/test splits (70/15/15)
  - [ ] Handle class imbalance with SMOTE or class weights
  - [ ] Implement data quality checks
  - [ ] Create training data snapshot for reproducibility
  - [ ] Add data lineage tracking

- [ ] **Task 3: Model Training Service** (AC: 2, 3, 4)
  - [ ] Create MLTrainingService class
  - [ ] Implement Logistic Regression training
  - [ ] Implement Random Forest training
  - [ ] Implement XGBoost training
  - [ ] Add hyperparameter tuning with grid search
  - [ ] Implement cross-validation (5-fold)
  - [ ] Create model comparison logic
  - [ ] Add early stopping for gradient boosting models
  - [ ] Implement training job scheduler (weekly cron)
  - [ ] Create training configuration management

- [ ] **Task 4: Model Evaluation and Selection** (AC: 4, 5, 6)
  - [ ] Calculate performance metrics (AUC-ROC, precision, recall, F1)
  - [ ] Generate confusion matrix
  - [ ] Calculate feature importance scores
  - [ ] Create calibration plots
  - [ ] Implement model selection criteria (weighted score)
  - [ ] Generate evaluation reports
  - [ ] Store evaluation results in ml_model_metadata
  - [ ] Create model comparison dashboard
  - [ ] Add statistical significance testing

- [ ] **Task 5: Model Registry and Versioning** (AC: 7)
  - [ ] Create model storage service (S3 or local filesystem)
  - [ ] Implement model serialization (pickle or joblib)
  - [ ] Create model registry with metadata tracking
  - [ ] Add model versioning (semantic versioning)
  - [ ] Implement model lineage tracking
  - [ ] Create model download/upload APIs
  - [ ] Add model rollback capability
  - [ ] Implement model archival for old versions

- [ ] **Task 6: Model Deployment Pipeline** (AC: 8, 14)
  - [ ] Create model deployment service
  - [ ] Implement canary deployment (5% traffic first)
  - [ ] Add performance threshold gates (min AUC > 0.75)
  - [ ] Create model loading and warm-up logic
  - [ ] Implement model caching for fast inference
  - [ ] Add prediction API with <100ms latency
  - [ ] Create health check endpoint for model status
  - [ ] Implement blue-green deployment for zero downtime

- [ ] **Task 7: A/B Testing Framework** (AC: 9)
  - [ ] Create experiment configuration system
  - [ ] Implement traffic splitting logic (50/50 or custom)
  - [ ] Track predictions from both models
  - [ ] Calculate statistical significance of results
  - [ ] Create experiment dashboard
  - [ ] Implement automatic winner selection
  - [ ] Add experiment termination logic
  - [ ] Create experiment result reports

- [ ] **Task 8: Model Monitoring and Drift Detection** (AC: 10, 11, 12)
  - [ ] Log all predictions with features and results
  - [ ] Track prediction distribution over time
  - [ ] Calculate model performance on recent data
  - [ ] Implement data drift detection (KS test, PSI)
  - [ ] Implement concept drift detection (accuracy degradation)
  - [ ] Create drift alert system (Slack, email)
  - [ ] Add automatic retraining trigger (drift > 0.1)
  - [ ] Create monitoring dashboard
  - [ ] Implement anomaly detection for predictions

- [ ] **Task 9: Model Explainability** (AC: 15)
  - [ ] Integrate SHAP library for model explanations
  - [ ] Calculate SHAP values for each prediction
  - [ ] Create feature contribution visualization
  - [ ] Implement top N feature extraction
  - [ ] Add explanation to prediction API response
  - [ ] Create global feature importance dashboard
  - [ ] Implement counterfactual explanations
  - [ ] Add explanation caching for performance

- [ ] **Task 10: Testing and Validation** (AC: 14)
  - [ ] Create mock training data generator
  - [ ] Write unit tests for feature engineering (>80% coverage)
  - [ ] Create integration tests for training pipeline
  - [ ] Test model serialization/deserialization
  - [ ] Benchmark inference latency (target <100ms)
  - [ ] Test drift detection with synthetic data
  - [ ] Validate A/B testing logic
  - [ ] Load test prediction API (1000 req/s)

## Dev Notes

### Architecture References
**Source Tree Context:**
```
backend/src/
├── services/ml/
│   ├── training.service.ts           # ML training orchestration
│   ├── feature-engineering.service.ts # Feature extraction
│   ├── model-registry.service.ts     # Model versioning
│   ├── prediction.service.ts         # Model inference
│   ├── monitoring.service.ts         # Drift detection
│   └── explainability.service.ts     # SHAP explanations
├── models/ml/
│   ├── logistic-regression.model.ts  # LR implementation
│   ├── random-forest.model.ts        # RF implementation
│   └── xgboost.model.ts              # XGBoost implementation
├── jobs/
│   ├── model-training.job.ts         # Weekly training job
│   └── drift-detection.job.ts        # Daily monitoring job
└── api/
    └── ml-prediction.controller.ts   # Prediction endpoints
```

### Python ML Environment
**Required Packages:**
```requirements.txt
scikit-learn==1.3.0
xgboost==1.7.6
pandas==2.0.3
numpy==1.24.3
shap==0.42.1
imbalanced-learn==0.11.0
joblib==1.3.2
scipy==1.11.1
```

**Install:**
```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Feature Engineering
**Feature Categories:**
```typescript
interface LeadFeatures {
  // BANT Score Components
  budget_score: number;         // 0-1, normalized budget adequacy
  authority_score: number;      // 0-1, decision maker confidence
  need_score: number;          // 0-1, requirement match quality
  timeline_score: number;       // 0-1, urgency indicator
  
  // Engagement Features
  interaction_count: number;    // Total interactions
  days_since_last_contact: number;
  interaction_frequency: number; // Interactions per week
  email_open_rate: number;      // % of emails opened
  response_rate: number;        // % of messages replied to
  
  // Velocity Features
  days_in_current_status: number;
  status_change_count: number;
  avg_days_per_status: number;
  
  // Property Preferences
  budget_min_normalized: number;
  budget_max_normalized: number;
  bedrooms_score: number;
  location_preference_score: number;
  
  // Demographic Features
  source_quality_score: number;  // Based on historical source performance
  property_type_demand: number;  // Market demand for preferred type
  
  // Temporal Features
  days_since_created: number;
  day_of_week_created: number;  // 0-6
  month_created: number;         // 1-12
  
  // Historical Features
  similar_lead_conversion_rate: number; // Rate for similar leads
}
```

**Feature Extraction Example:**
```typescript
async function extractFeatures(lead: Lead): Promise<LeadFeatures> {
  return {
    budget_score: normalizeBudget(lead.budgetMax),
    interaction_count: await countInteractions(lead.leadId),
    days_since_last_contact: calculateDaysSince(lead.lastContactedAt),
    // ... extract all features
  };
}
```

### Model Training Configuration
**Training Config:**
```typescript
interface TrainingConfig {
  models: {
    logistic_regression: {
      penalty: 'l2',
      C: 1.0,
      solver: 'lbfgs',
      max_iter: 1000
    },
    random_forest: {
      n_estimators: 100,
      max_depth: 10,
      min_samples_split: 5,
      min_samples_leaf: 2
    },
    xgboost: {
      n_estimators: 100,
      learning_rate: 0.1,
      max_depth: 6,
      subsample: 0.8,
      colsample_bytree: 0.8
    }
  },
  validation: {
    cv_folds: 5,
    test_size: 0.15,
    stratify: true
  },
  selection_criteria: {
    primary_metric: 'auc_roc',
    secondary_metrics: ['precision', 'recall', 'f1'],
    min_auc_threshold: 0.75
  }
}
```

### Training Pipeline Flow
1. **Data Collection:**
   ```sql
   SELECT l.*, 
          CASE WHEN l.status = 'Closed Won' THEN 1 ELSE 0 END as converted
   FROM leads l
   WHERE l.created_at >= NOW() - INTERVAL '6 months'
     AND l.status IN ('Closed Won', 'Closed Lost')
   LIMIT 10000;
   ```

2. **Feature Engineering:**
   - Extract features for each lead
   - Handle missing values (imputation or default)
   - Normalize numerical features (StandardScaler)
   - Encode categorical features (one-hot or label encoding)

3. **Train/Test Split:**
   - 70% training, 15% validation, 15% test
   - Stratified sampling to maintain class balance

4. **Model Training:**
   - Train each model with hyperparameter tuning
   - Use 5-fold cross-validation
   - Track training metrics

5. **Model Evaluation:**
   - Calculate metrics on test set
   - Generate feature importance
   - Create visualizations

6. **Model Selection:**
   - Compare models on validation set
   - Select best based on AUC-ROC
   - Verify meets minimum threshold (0.75)

7. **Model Deployment:**
   - Save model to registry
   - Deploy to staging for A/B test
   - Promote to production after validation

### Prediction API
**Endpoint:**
```typescript
POST /api/v1/ml/predict

Request:
{
  "leadId": 123
}

Response:
{
  "leadId": 123,
  "prediction": {
    "score": 0.85,
    "probability": 0.847,
    "class": "high_potential",
    "confidence": 0.92
  },
  "explanation": {
    "top_features": [
      { "feature": "budget_score", "value": 0.9, "contribution": 0.15 },
      { "feature": "interaction_count", "value": 12, "contribution": 0.12 },
      { "feature": "days_since_last_contact", "value": 2, "contribution": 0.08 }
    ]
  },
  "model_version": "v2.3.1",
  "timestamp": "2024-09-30T12:00:00Z"
}
```

### Model Monitoring
**Metrics to Track:**
```typescript
interface ModelMetrics {
  // Performance Metrics
  auc_roc: number;
  precision: number;
  recall: number;
  f1_score: number;
  
  // Distribution Metrics
  prediction_mean: number;
  prediction_std: number;
  prediction_quartiles: number[];
  
  // Drift Metrics
  feature_drift_scores: Record<string, number>; // PSI per feature
  concept_drift_score: number;                  // Recent accuracy vs baseline
  
  // System Metrics
  avg_inference_time_ms: number;
  p95_inference_time_ms: number;
  predictions_per_second: number;
}
```

**Drift Detection:**
```typescript
// Population Stability Index (PSI)
function calculatePSI(expected: number[], actual: number[]): number {
  const bins = 10;
  const expectedHist = histogram(expected, bins);
  const actualHist = histogram(actual, bins);
  
  let psi = 0;
  for (let i = 0; i < bins; i++) {
    const exp = expectedHist[i] || 0.0001; // Avoid divide by zero
    const act = actualHist[i] || 0.0001;
    psi += (act - exp) * Math.log(act / exp);
  }
  
  return psi;
}

// Alert thresholds
const DRIFT_THRESHOLDS = {
  low: 0.1,    // Minor drift, monitor
  medium: 0.2, // Significant drift, investigate
  high: 0.3    // Severe drift, retrain immediately
};
```

### SHAP Explanations
**SHAP Integration:**
```python
import shap

def explain_prediction(model, features):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(features)
    
    return {
        'base_value': explainer.expected_value,
        'shap_values': shap_values.tolist(),
        'feature_names': features.columns.tolist(),
        'feature_values': features.values.tolist()
    }
```

### Database Schema Usage
**Tables:**
- `lead_features` - Feature store with versioning
- `ml_models` - Model registry with metadata
- `ml_model_metadata` - Training configuration and metrics
- `lead_scores` - Prediction history
- `feature_importance` - Feature importance per model

**Key Indexes:**
```sql
CREATE INDEX idx_lead_features_lead_id ON lead_features(lead_id);
CREATE INDEX idx_ml_models_version ON ml_models(version, is_active);
CREATE INDEX idx_lead_scores_scored_at ON lead_scores(scored_at DESC);
```

### Performance Optimization
- **Feature Caching:** Cache extracted features for 1 hour
- **Model Caching:** Keep loaded model in memory
- **Batch Predictions:** Process multiple leads in one call
- **Async Training:** Run training in background workers
- **Incremental Learning:** Update model with new data without full retrain

### Testing
**Test File Location:** `backend/src/__tests__/ml/`
**Testing Framework:** Jest with Python subprocess for ML code

**Key Test Scenarios:**
1. Feature extraction with various lead data
2. Model training with synthetic data
3. Prediction accuracy with known outcomes
4. Drift detection with changing distributions
5. Latency benchmarking (<100ms requirement)
6. A/B test traffic splitting
7. Model serialization/deserialization
8. Explanation generation

### Monitoring and Alerts
**Metrics Dashboard:**
- Model performance trends (daily AUC-ROC)
- Feature drift scores
- Prediction distribution
- Inference latency (p50, p95, p99)
- Training job success/failure

**Alerts:**
- AUC-ROC drops below 0.75
- Feature drift PSI > 0.2
- Inference latency > 100ms (p95)
- Training job fails
- Prediction anomalies detected

### Security Considerations
- Validate all input features before prediction
- Sanitize feature values to prevent injection
- Encrypt model files at rest
- Implement prediction rate limiting
- Audit log all model deployments
- Restrict model management to admin users only

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-09-30 | 1.0 | Initial story creation | AI Assistant |

## Dev Agent Record
*This section will be populated during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*To be filled by QA agent after implementation*
